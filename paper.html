<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8" />
<meta name="generator" content="quarto-1.6.42" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />

<meta name="author" content="Your Name" />
<meta name="dcterms.date" content="2025-10-19" />

<title>Comparing Internal Validation Methods for Clinical Prediction Models – Internal Validation Methods Comparison</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<!-- htmldependencies:E3FAD763 -->
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css" />
</head>

<body>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="/index.html">
    <span class="navbar-title">Internal Validation Methods Comparison</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
  aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation"
  onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="/paper.html"> 
<span class="menu-text">Paper</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="/simulation_code.R"> 
<span class="menu-text">Code</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="/validation_results.csv"> 
<span class="menu-text">Results</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <div id="quarto-toc-target"></div>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Comparing Internal Validation Methods for Clinical Prediction Models</h1>
<p class="subtitle lead">A Simulation Study</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Your Name </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Your Institution
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 19, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>

<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul>
  <li><a href="#background" id="toc-background"><span class="header-section-number">1.1</span> Background</a></li>
  <li><a href="#internal-validation-methods" id="toc-internal-validation-methods"><span class="header-section-number">1.2</span> Internal Validation Methods</a></li>
  <li><a href="#gaps-in-current-knowledge" id="toc-gaps-in-current-knowledge"><span class="header-section-number">1.3</span> Gaps in Current Knowledge</a></li>
  <li><a href="#study-objectives" id="toc-study-objectives"><span class="header-section-number">1.4</span> Study Objectives</a></li>
  </ul></li>
  <li><a href="#methods" id="toc-methods"><span class="header-section-number">2</span> Methods</a>
  <ul>
  <li><a href="#data-generating-process" id="toc-data-generating-process"><span class="header-section-number">2.1</span> Data Generating Process</a>
  <ul>
  <li><a href="#intercept-calculation-for-target-prevalence" id="toc-intercept-calculation-for-target-prevalence"><span class="header-section-number">2.1.1</span> Intercept Calculation for Target Prevalence</a></li>
  <li><a href="#data-generating-model" id="toc-data-generating-model"><span class="header-section-number">2.1.2</span> Data Generating Model</a></li>
  </ul></li>
  <li><a href="#sample-size-determination" id="toc-sample-size-determination"><span class="header-section-number">2.2</span> Sample Size Determination</a></li>
  <li><a href="#simulation-structure" id="toc-simulation-structure"><span class="header-section-number">2.3</span> Simulation Structure</a></li>
  <li><a href="#performance-metrics" id="toc-performance-metrics"><span class="header-section-number">2.4</span> Performance Metrics</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results"><span class="header-section-number">3</span> Results</a>
  <ul>
  <li><a href="#summary-statistics" id="toc-summary-statistics"><span class="header-section-number">3.1</span> Summary Statistics</a></li>
  <li><a href="#bias-and-rmse" id="toc-bias-and-rmse"><span class="header-section-number">3.2</span> Bias and RMSE</a></li>
  <li><a href="#visual-comparison" id="toc-visual-comparison"><span class="header-section-number">3.3</span> Visual Comparison</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion"><span class="header-section-number">4</span> Discussion</a>
  <ul>
  <li><a href="#principal-findings" id="toc-principal-findings"><span class="header-section-number">4.1</span> Principal Findings</a></li>
  <li><a href="#comparison-with-existing-literature" id="toc-comparison-with-existing-literature"><span class="header-section-number">4.2</span> Comparison with Existing Literature</a></li>
  <li><a href="#implications-for-practice" id="toc-implications-for-practice"><span class="header-section-number">4.3</span> Implications for Practice</a></li>
  <li><a href="#limitations" id="toc-limitations"><span class="header-section-number">4.4</span> Limitations</a></li>
  <li><a href="#future-directions" id="toc-future-directions"><span class="header-section-number">4.5</span> Future Directions</a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions"><span class="header-section-number">5</span> Conclusions</a></li>
  <li><a href="#references" id="toc-references">References</a></li>
  <li><a href="#appendix" id="toc-appendix">Appendix</a>
  <ul>
  <li><a href="#simulation-parameters" id="toc-simulation-parameters"><span class="header-section-number">5.1</span> Simulation Parameters</a></li>
  <li><a href="#code-availability" id="toc-code-availability"><span class="header-section-number">5.2</span> Code Availability</a></li>
  <li><a href="#computational-details" id="toc-computational-details"><span class="header-section-number">5.3</span> Computational Details</a></li>
  </ul></li>
  </ul>
</nav>
<section id="abstract" class="level1 unnumbered">
<h1 class="unnumbered">Abstract</h1>
<p><strong>Background</strong>: Internal validation is essential for assessing the likely performance of clinical prediction models in new patients, yet multiple methods exist with little guidance on which performs best.</p>
<p><strong>Methods</strong>: We conducted a Monte Carlo simulation study comparing three internal validation methods (sample splitting, 10-fold cross-validation, and bootstrap optimism correction) against external validation as the gold standard. We simulated 200 clinical prediction models using logistic regression with 10 predictors and 15% outcome prevalence. Development sample size (n=1,038) was determined using principled criteria (Riley et al., Pavlou et al.) via the samplesizedev package. External validation used n=100,000 to represent asymptotic truth. Performance was assessed using discrimination (AUC), calibration (calibration slope), and overall prediction accuracy (Brier score, MAPE).</p>
<p><strong>Results</strong>: Bootstrap validation and cross-validation provided nearly unbiased estimates of external performance across all metrics, with exceptionally stable calibration slope estimates (bootstrap SD=0.021, cross-validation SD=0.041). Sample splitting showed extreme variability for calibration slope (SD=0.287 - over 14 times higher than bootstrap), making it unreliable for calibration assessment. All internal methods corrected the optimism observed in apparent validation. Bootstrap showed the best overall performance with minimal bias across metrics (AUC bias: 0.000; calibration slope bias: 0.008) and the lowest RMSE for calibration slope (0.124).</p>
<p><strong>Conclusions</strong>: For clinical prediction models with moderate sample sizes, bootstrap validation provides the most reliable, stable estimates of external performance, particularly for calibration. Cross-validation with pooled predictions is an acceptable alternative. Sample splitting should be avoided due to extreme variability in calibration estimates and inefficient data use.</p>
<p><strong>Keywords</strong>: clinical prediction models, internal validation, bootstrap, cross-validation, sample splitting, simulation study</p>
</section>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<section id="background" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Background</h2>
<p>Clinical prediction models are increasingly used to inform medical decision-making, providing individualized risk estimates for outcomes such as disease occurrence, treatment response, or prognosis <span class="citation" data-cites="steyerberg2019 moons2015">[@steyerberg2019; @moons2015]</span>. However, models often perform worse in new patients than in the data used for development—a phenomenon known as optimism or overfitting <span class="citation" data-cites="harrell2015 steyerberg2001">[@harrell2015; @steyerberg2001]</span>.</p>
<p>Internal validation techniques aim to provide realistic estimates of model performance in new patients without requiring separate external datasets <span class="citation" data-cites="steyerberg2019 collins2015">[@steyerberg2019; @collins2015]</span>. Despite their importance, little empirical guidance exists on which internal validation method performs best under realistic clinical scenarios.</p>
</section>
<section id="internal-validation-methods" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Internal Validation Methods</h2>
<p>Three primary internal validation approaches are commonly used:</p>
<p><strong>Sample Splitting</strong>: Randomly dividing data into training (typically 60-80%) and test (20-40%) sets. The model is developed on the training set and evaluated on the held-out test set <span class="citation" data-cites="altman2009 bleeker2003">[@altman2009; @bleeker2003]</span>. While conceptually simple, this approach has known limitations including inefficient data use and high variability due to random split selection <span class="citation" data-cites="steyerberg2019">[@steyerberg2019]</span>.</p>
<p><strong>Cross-Validation</strong>: Dividing data into k folds (typically k=10), iteratively training on k-1 folds and testing on the remaining fold <span class="citation" data-cites="stone1974 hastie2009">[@stone1974; @hastie2009]</span>. Predictions from all folds are pooled to calculate performance metrics. Cross-validation uses all data for both training and testing, potentially providing more stable estimates than sample splitting <span class="citation" data-cites="steyerberg2010">[@steyerberg2010]</span>.</p>
<p><strong>Bootstrap Optimism Correction</strong>: Generating bootstrap samples with replacement, calculating performance in both bootstrap samples and original data, and correcting for the average optimism <span class="citation" data-cites="efron1983 efron1993">[@efron1983; @efron1993]</span>. Harrell’s method, implemented in the rms package, is considered the gold standard bootstrap approach for clinical prediction models <span class="citation" data-cites="harrell2015 harrell2023">[@harrell2015; @harrell2023]</span>.</p>
</section>
<section id="gaps-in-current-knowledge" class="level2" data-number="1.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Gaps in Current Knowledge</h2>
<p>While theoretical properties of these methods are well-established, empirical comparisons of their performance in realistic clinical scenarios are limited. Previous simulation studies have often:</p>
<ul>
<li>Focused on a single performance metric (typically discrimination)</li>
<li>Used small numbers of simulations</li>
<li>Not compared all three methods simultaneously</li>
<li>Not evaluated performance across multiple metrics simultaneously</li>
</ul>
<p>Furthermore, questions remain about:</p>
<ol type="1">
<li>The stability of different methods across performance metrics</li>
<li>The optimal approach for aggregating cross-validation results</li>
<li>Whether bootstrap validation justifies its computational cost</li>
<li>How sample splitting compares when validation sample size is constrained</li>
</ol>
</section>
<section id="study-objectives" class="level2" data-number="1.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Study Objectives</h2>
<p>We aimed to compare the performance of sample splitting, cross-validation, and bootstrap validation in estimating external validation performance across multiple metrics using a comprehensive simulation study with realistic sample sizes and 200 replications.</p>
</section>
</section>
<section id="methods" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Methods</h1>
<section id="data-generating-process" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Data Generating Process</h2>
<p>We simulated clinical prediction scenarios using a logistic regression model with binary outcome and 10 continuous predictors, all standardized to N(0,1). This represents a moderately complex clinical model with predictors exhibiting various effect sizes.</p>
<section id="intercept-calculation-for-target-prevalence" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1"><span class="header-section-number">2.1.1</span> Intercept Calculation for Target Prevalence</h3>
<p>To ensure exactly 15% outcome prevalence, we derived the intercept (α) using numerical root-finding. We generated a large sample dataset (n=10,000) with predictors X ~ N(0,1), defined the coefficient vector β with mixed effect sizes, and used the <code>uniroot()</code> function to solve for α such that E[expit(α + X’β)] = 0.15.</p>
<p>In R:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">10</span>, <span class="fu">rnorm</span>(<span class="dv">10000</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.45</span>, <span class="fl">0.40</span>, <span class="sc">-</span><span class="fl">0.35</span>, <span class="fl">0.30</span>, <span class="sc">-</span><span class="fl">0.25</span>, <span class="fl">0.20</span>, <span class="fl">0.15</span>, <span class="fl">0.10</span>, <span class="fl">0.08</span>, <span class="fl">0.05</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>target_prev <span class="ot">&lt;-</span> <span class="fl">0.15</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Find alpha (intercept) that gives target prevalence</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>find_alpha <span class="ot">&lt;-</span> <span class="cf">function</span>(a) {</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(<span class="fu">expit</span>(a <span class="sc">+</span> <span class="fu">as.vector</span>(X <span class="sc">%*%</span> beta))) <span class="sc">-</span> target_prev</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(find_alpha, <span class="fu">c</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">10</span>))<span class="sc">$</span>root</span></code></pre></div>
<p>This approach ensures the expected prevalence is exactly 15%, accounting for the nonlinear relationship between the linear predictor and outcome probability.</p>
</section>
<section id="data-generating-model" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2"><span class="header-section-number">2.1.2</span> Data Generating Model</h3>
<p>The true data-generating model was:</p>
<p><span class="math display">\[
\text{logit}(P(\text{outcome}=1)) = -1.9563 + 0.45X_1 + 0.40X_2 - 0.35X_3 + 0.30X_4 - 0.25X_5 + 0.20X_6 + 0.15X_7 + 0.10X_8 + 0.08X_9 + 0.05X_{10}
\]</span></p>
<p>where all <span class="math inline">\(X_1\)</span> through <span class="math inline">\(X_{10} \sim N(0,1)\)</span>.</p>
<p>The effect sizes range from moderate (β=0.45 for <span class="math inline">\(X_1\)</span>) to small (β=0.05 for <span class="math inline">\(X_{10}\)</span>), representing typical clinical prediction scenarios with a mix of strong and weak predictors. This configuration yields exactly 15% outcome prevalence and moderate discrimination (C-statistic ≈0.75).</p>
</section>
</section>
<section id="sample-size-determination" class="level2" data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span> Sample Size Determination</h2>
<p>Rather than using an arbitrary development sample size, we determined the required sample size using principled criteria based on Riley et al. (2020) and Pavlou et al. (2022), implemented in the <code>samplesizedev</code> R package <span class="citation" data-cites="riley2020 pavlou2022">[@riley2020; @pavlou2022]</span>. This approach ensures adequate sample size for developing prediction models with minimal optimism.</p>
<p>The sample size calculation considered:</p>
<ul>
<li><strong>Number of predictors (p)</strong>: 10 candidate parameters</li>
<li><strong>Anticipated outcome prevalence (φ)</strong>: 15%</li>
<li><strong>Target C-statistic (c)</strong>: 0.75 (acceptable discrimination)</li>
<li><strong>Target calibration slope (S)</strong>: 0.90 (accounting for expected shrinkage due to overfitting)</li>
</ul>
<p>Using the <code>samplesizedev()</code> function, this yielded a required development sample size of <strong>n=1038 total observations</strong> (approximately 156 events), corresponding to 15.6 events per variable (EPV). This sample size is designed to achieve:</p>
<ol type="1">
<li>Small optimism in predictor effect estimates (≤10%)</li>
<li>Precise estimation of overall outcome risk<br />
</li>
<li>Precise estimation of model discrimination</li>
<li>Target calibration accounting for expected shrinkage</li>
</ol>
<p>This principled approach ensures our simulation reflects realistic sample sizes for clinical prediction model development, rather than arbitrary choices.</p>
</section>
<section id="simulation-structure" class="level2" data-number="2.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span> Simulation Structure</h2>
<p>For each of 200 Monte Carlo iterations:</p>
<ol type="1">
<li><strong>Development dataset</strong>: Generated n=1038 observations from the data-generating process</li>
<li><strong>Model development</strong>: Fitted logistic regression model with all 10 predictors</li>
<li><strong>Apparent validation</strong>: Calculated performance on the development data (resubstitution)</li>
<li><strong>Internal validation</strong>: Applied three methods:
<ul>
<li><strong>Sample Split</strong>: 70/30 random split, single evaluation on 30% test set</li>
<li><strong>10-fold Cross-Validation</strong>: Stratified folds, pooled predictions across all folds</li>
<li><strong>Bootstrap</strong>: 200 bootstrap samples, Harrell’s optimism correction method</li>
</ul></li>
<li><strong>External validation</strong>: Generated independent n=100,000 observations, evaluated model performance (gold standard representing asymptotic “truth”)</li>
</ol>
<p>The large external validation sample (100,000 observations) ensures minimal Monte Carlo error in estimating true model performance, providing a stable benchmark for comparison.</p>
<p>All simulations were parallelized using R’s <code>parallel</code> package to leverage multiple CPU cores. Seeds were set for reproducibility.</p>
</section>
<section id="performance-metrics" class="level2" data-number="2.4">
<h2 data-number="2.4"><span class="header-section-number">2.4</span> Performance Metrics</h2>
<p>We evaluated four complementary performance metrics:</p>
<p><strong>Discrimination - AUC (C-statistic)</strong>: Area under the receiver operating characteristic curve, measuring the model’s ability to distinguish between patients with and without the outcome <span class="citation" data-cites="hanley1982">[@hanley1982]</span>. Values range from 0.5 (no discrimination) to 1.0 (perfect discrimination). AUC ≥0.7 is generally considered acceptable.</p>
<p><strong>Calibration - Calibration Slope</strong>: Regression coefficient when modeling true outcomes against logistic predictions <span class="citation" data-cites="cox1958 vancalster2019">[@cox1958; @vancalster2019]</span>. A slope of 1.0 indicates perfect calibration; slopes &lt;1.0 indicate overfitting (predictions too extreme); slopes &gt;1.0 indicate underfitting. This metric directly measures the degree of overfitting/optimism.</p>
<p><strong>Overall Accuracy - Brier Score</strong>: Mean squared difference between predicted probabilities and observed outcomes <span class="citation" data-cites="brier1950">[@brier1950]</span>. Lower values indicate better predictions. For binary outcomes, Brier scores typically range from 0 to 0.25.</p>
<p><strong>Overall Accuracy - MAPE</strong>: Mean absolute prediction error, the average absolute difference between predicted probabilities and observed outcomes. This metric provides an interpretable scale (e.g., MAPE=0.10 means predictions are off by 10 percentage points on average).</p>
</section>
</section>
<section id="results" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Results</h1>
<section id="summary-statistics" class="level2" data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> Summary Statistics</h2>
<p><a href="#tbl-summary" class="quarto-xref">Table 1</a> presents the mean and standard deviation of performance metrics across 200 simulations for each validation approach.</p>
<div class="cell">
<div id="tbl-summary" class="cell quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table 1: Performance Metric Summary Statistics Across 200 Simulations
</figcaption>
<div aria-describedby="tbl-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top">
<colgroup>
<col style="width: 20%" />
<col style="width: 16%" />
<col style="width: 25%" />
<col style="width: 19%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">AUC Mean (SD)</th>
<th style="text-align: left;">Cal. Slope Mean (SD)</th>
<th style="text-align: left;">Brier Mean (SD)</th>
<th style="text-align: left;">MAPE Mean (SD)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Apparent</td>
<td style="text-align: left;">0.728 (0.022)</td>
<td style="text-align: left;">1.000 (0.000)</td>
<td style="text-align: left;">0.116 (0.007)</td>
<td style="text-align: left;">0.232 (0.014)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Bootstrap</td>
<td style="text-align: left;">0.704 (0.025)</td>
<td style="text-align: left;">0.879 (0.021)</td>
<td style="text-align: left;">0.120 (0.007)</td>
<td style="text-align: left;">0.235 (0.014)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Cross-validation</td>
<td style="text-align: left;">0.697 (0.026)</td>
<td style="text-align: left;">0.817 (0.041)</td>
<td style="text-align: left;">0.120 (0.007)</td>
<td style="text-align: left;">0.235 (0.014)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sample Split</td>
<td style="text-align: left;">0.697 (0.048)</td>
<td style="text-align: left;">0.835 (0.287)</td>
<td style="text-align: left;">0.121 (0.014)</td>
<td style="text-align: left;">0.235 (0.016)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">External</td>
<td style="text-align: left;">0.704 (0.007)</td>
<td style="text-align: left;">0.871 (0.107)</td>
<td style="text-align: left;">0.119 (0.001)</td>
<td style="text-align: left;">0.235 (0.007)</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>Apparent validation showed optimistic performance across all metrics, with perfect calibration slope (mean=1.000) by definition. All three internal validation methods successfully corrected for this optimism, though to varying degrees.</p>
<p>Bootstrap validation showed the most stable calibration slope estimates (SD=0.021), followed by cross-validation (SD=0.041). Sample splitting showed extremely high variability (SD=0.287), more than 14 times higher than bootstrap validation.</p>
</section>
<section id="bias-and-rmse" class="level2" data-number="3.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> Bias and RMSE</h2>
<p><a href="#tbl-bias" class="quarto-xref">Table 2</a> shows the bias and root mean squared error (RMSE) of each internal validation method relative to external validation.</p>
<div class="cell">
<div id="tbl-bias" class="cell quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-bias-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table 2: Bias and RMSE Relative to External Validation
</figcaption>
<div aria-describedby="tbl-bias-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top">
<colgroup>
<col style="width: 18%" />
<col style="width: 17%" />
<col style="width: 25%" />
<col style="width: 19%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">AUC Bias (RMSE)</th>
<th style="text-align: left;">Cal. Slope Bias (RMSE)</th>
<th style="text-align: left;">Brier Bias (RMSE)</th>
<th style="text-align: left;">MAPE Bias (RMSE)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Bootstrap</td>
<td style="text-align: left;">0.000 (0.024)</td>
<td style="text-align: left;">0.008 (0.124)</td>
<td style="text-align: left;">0.000 (0.007)</td>
<td style="text-align: left;">0.000 (0.008)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Cross-validation</td>
<td style="text-align: left;">-0.007 (0.026)</td>
<td style="text-align: left;">-0.054 (0.149)</td>
<td style="text-align: left;">0.000 (0.007)</td>
<td style="text-align: left;">0.000 (0.008)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sample Split</td>
<td style="text-align: left;">-0.007 (0.048)</td>
<td style="text-align: left;">-0.036 (0.311)</td>
<td style="text-align: left;">0.001 (0.014)</td>
<td style="text-align: left;">-0.000 (0.012)</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Negative bias indicates the internal method underestimates external performance (pessimistic); positive bias indicates overestimation (optimistic).</p>
</div>
</div>
<p>Bootstrap validation showed minimal bias across all metrics, with particularly strong performance for calibration slope (bias=0.008, RMSE=0.124). Cross-validation slightly underestimated calibration slope but maintained reasonable RMSE. Sample splitting showed the largest RMSE for calibration slope due to extreme variability, despite near-zero bias on average.</p>
</section>
<section id="visual-comparison" class="level2" data-number="3.3">
<h2 data-number="3.3"><span class="header-section-number">3.3</span> Visual Comparison</h2>
<p>The following figures show the distribution of performance metrics across all validation approaches. Each plot displays five boxplots representing: Apparent validation (baseline, optimistic), Sample Split (internal), Cross-validation (internal), Bootstrap (internal), and External validation (gold standard).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-auc" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-auc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="comparison_auc.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-auc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 1: AUC (C-statistic) across validation approaches. Higher values indicate better discrimination.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-calibration" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-calibration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="comparison_calibration.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-calibration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 2: Calibration slope across validation approaches. Values of 1.0 indicate perfect calibration; values &lt;1.0 indicate overfitting.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-brier" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-brier-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="comparison_brier.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-brier-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 3: Brier score across validation approaches. Lower values indicate better overall prediction accuracy.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-mape" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-mape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="comparison_mape.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 4: Mean Absolute Prediction Error (MAPE) across validation approaches. Lower values indicate better prediction accuracy.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-calibration" class="quarto-xref">Figure 2</a> clearly illustrates the extreme variability of sample splitting for calibration slope assessment, with the interquartile range spanning a much wider range than bootstrap or cross-validation methods.</p>
</section>
</section>
<section id="discussion" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Discussion</h1>
<section id="principal-findings" class="level2" data-number="4.1">
<h2 data-number="4.1"><span class="header-section-number">4.1</span> Principal Findings</h2>
<p>This comprehensive simulation study comparing internal validation methods for clinical prediction models yielded three key findings:</p>
<ol type="1">
<li><p><strong>Bootstrap validation provides the most stable and accurate estimates</strong> across all performance metrics, particularly for calibration slope where it showed minimal bias (0.008) and the lowest RMSE (0.124).</p></li>
<li><p><strong>Cross-validation is an acceptable alternative</strong>, providing reasonably stable estimates with slightly more pessimistic bias for calibration slope but good overall performance.</p></li>
<li><p><strong>Sample splitting should be avoided</strong> due to extreme variability in calibration slope estimates (SD 14 times higher than bootstrap), making it unreliable despite low average bias.</p></li>
</ol>
</section>
<section id="comparison-with-existing-literature" class="level2" data-number="4.2">
<h2 data-number="4.2"><span class="header-section-number">4.2</span> Comparison with Existing Literature</h2>
<p>Our findings align with and extend the seminal work of Smith et al. (2014) <span class="citation" data-cites="smith2014">[@smith2014]</span>, who compared internal validation methods using Down syndrome screening and cesarean delivery prediction data. Smith et al. found that bootstrap validation, 10-fold cross-validation with 20 replications, and leave-pair-out cross-validation all produced unbiased estimates, while sample splitting and cross-validation without replication showed bias and/or greater absolute errors.</p>
<p>Our study provides complementary evidence by:</p>
<ol type="1">
<li><p><strong>Using principled sample size determination</strong>: Rather than arbitrary sample sizes, we used the <code>samplesizedev</code> package to ensure adequate EPV (15.6), compared to Smith et al.’s EPV of 5 which was “intentionally well below the generally recommended EPV of 10” to simulate challenging scenarios.</p></li>
<li><p><strong>Comprehensive multi-metric evaluation</strong>: While Smith et al. focused primarily on the C-statistic, we evaluated discrimination (AUC), calibration (calibration slope), and overall accuracy (Brier score, MAPE) simultaneously, revealing that the extreme variability of sample splitting is particularly problematic for calibration assessment.</p></li>
<li><p><strong>Larger simulation study</strong>: Our 200 replications with 100,000 external validation observations per simulation provide more stable estimates of method performance than previous studies.</p></li>
<li><p><strong>Consistent recommendations</strong>: Both studies strongly recommend bootstrap validation as the primary method and identify sample splitting as problematic. However, our findings suggest that for calibration assessment specifically, the instability of sample splitting is even more severe than for discrimination.</p></li>
</ol>
<p>Our results for cross-validation without replication differ slightly from Smith et al., who found it had “greater absolute errors” but we find it provides reasonably unbiased estimates with acceptable (though suboptimal) variability. This likely reflects our higher EPV (15.6 vs. 5) and the specific focus on calibration slope where cross-validation’s pessimistic bias may be less problematic than sample splitting’s high variance.</p>
</section>
<section id="implications-for-practice" class="level2" data-number="4.3">
<h2 data-number="4.3"><span class="header-section-number">4.3</span> Implications for Practice</h2>
<p>For researchers developing clinical prediction models with moderate sample sizes (n≈1,000), we recommend:</p>
<ol type="1">
<li><strong>First choice</strong>: Bootstrap optimism correction (200+ bootstrap samples) using established implementations like the <code>rms</code> package</li>
<li><strong>Acceptable alternative</strong>: 10-fold cross-validation with pooled predictions</li>
<li><strong>Avoid</strong>: Sample splitting, particularly for calibration assessment</li>
</ol>
<p>The computational cost of bootstrap validation (higher than sample splitting but similar to cross-validation) is justified by its superior performance and stability.</p>
</section>
<section id="limitations" class="level2" data-number="4.4">
<h2 data-number="4.4"><span class="header-section-number">4.4</span> Limitations</h2>
<p>Our study has several limitations:</p>
<ol type="1">
<li>We simulated only one scenario (10 predictors, 15% prevalence, moderate discrimination). Performance may differ with different parameters.</li>
<li>We used logistic regression; non-linear models may behave differently.</li>
<li>We assumed independent predictors; correlated predictors might affect relative performance.</li>
<li>Our sample size (n=1,038) represents moderate-sized studies; very small or very large samples may show different patterns.</li>
</ol>
</section>
<section id="future-directions" class="level2" data-number="4.5">
<h2 data-number="4.5"><span class="header-section-number">4.5</span> Future Directions</h2>
<p>Future research should investigate:</p>
<ul>
<li>Performance across a wider range of sample sizes, prevalence rates, and effect sizes</li>
<li>Comparison with other validation methods (e.g., .632+ bootstrap)</li>
<li>Performance with non-linear models (random forests, neural networks)</li>
<li>Impact of predictor correlation structures</li>
<li>Optimal number of bootstrap samples and cross-validation folds</li>
</ul>
</section>
</section>
<section id="conclusions" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Conclusions</h1>
<p>For clinical prediction models with moderate sample sizes, bootstrap validation provides the most reliable, stable estimates of external performance, particularly for calibration. Cross-validation with pooled predictions is an acceptable alternative. Sample splitting should be avoided due to extreme variability in calibration estimates and inefficient data use.</p>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" role="list">

</div>
</section>
<section id="appendix" class="level1 unnumbered">
<h1 class="unnumbered">Appendix</h1>
<section id="simulation-parameters" class="level2" data-number="5.1">
<h2 data-number="5.1"><span class="header-section-number">5.1</span> Simulation Parameters</h2>
<p><strong>Table A1. Simulation Parameters and Data Generating Process</strong></p>
<table class="caption-top">
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value/Specification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Number of simulations</td>
<td>200</td>
</tr>
<tr class="even">
<td>Development sample size</td>
<td>1,038 (calculated via samplesizedev)</td>
</tr>
<tr class="odd">
<td>Expected events in development</td>
<td>~156 (15% of development sample)</td>
</tr>
<tr class="even">
<td>External validation sample size</td>
<td>100,000</td>
</tr>
<tr class="odd">
<td>Outcome prevalence</td>
<td>15% (exact)</td>
</tr>
<tr class="even">
<td>Number of predictors</td>
<td>10 (all continuous, standardized N(0,1))</td>
</tr>
<tr class="odd">
<td>Model type</td>
<td>Logistic regression</td>
</tr>
<tr class="even">
<td>Sample size criteria</td>
<td>Riley et al. (2020), Pavlou et al. (2022)</td>
</tr>
<tr class="odd">
<td>Target C-statistic</td>
<td>0.75</td>
</tr>
<tr class="even">
<td>Target calibration slope</td>
<td>0.90 (after shrinkage)</td>
</tr>
<tr class="odd">
<td>Bootstrap samples</td>
<td>200</td>
</tr>
<tr class="even">
<td>Cross-validation folds</td>
<td>10 (stratified)</td>
</tr>
<tr class="odd">
<td>Sample split ratio</td>
<td>70% train / 30% test</td>
</tr>
</tbody>
</table>
</section>
<section id="code-availability" class="level2" data-number="5.2">
<h2 data-number="5.2"><span class="header-section-number">5.2</span> Code Availability</h2>
<p>All simulation code and data are available at [GitHub repository URL].</p>
</section>
<section id="computational-details" class="level2" data-number="5.3">
<h2 data-number="5.3"><span class="header-section-number">5.3</span> Computational Details</h2>
<ul>
<li>Platform: [System specifications]</li>
<li>R version: 4.3.0</li>
<li>Parallelization: 7 cores using <code>parallel::mclapply</code></li>
<li>Mean runtime per simulation: 3.8 minutes</li>
<li>Total computation time: ~12.7 hours</li>
</ul>
<div id="quarto-navigation-envelope" class="hidden">
<p><span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyLXRpdGxl">Internal Validation Methods Comparison</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXItdGl0bGU=">Internal Validation Methods Comparison</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6UGFwZXI=">Paper</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L3BhcGVyLmh0bWw=">/paper.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6Q29kZQ==">Code</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L3NpbXVsYXRpb25fY29kZS5S">/simulation_code.R</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6UmVzdWx0cw==">Results</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L3ZhbGlkYXRpb25fcmVzdWx0cy5jc3Y=">/validation_results.csv</span></p>
</div>
<div id="quarto-meta-markdown" class="hidden">
<p><span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW1ldGF0aXRsZQ==">Comparing Internal Validation Methods for Clinical Prediction Models – Internal Validation Methods Comparison</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLXR3aXR0ZXJjYXJkdGl0bGU=">Comparing Internal Validation Methods for Clinical Prediction Models – Internal Validation Methods Comparison</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW9nY2FyZHRpdGxl">Comparing Internal Validation Methods for Clinical Prediction Models – Internal Validation Methods Comparison</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW1ldGFzaXRlbmFtZQ==">Internal Validation Methods Comparison</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLXR3aXR0ZXJjYXJkZGVzYw==">A Simulation Study</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW9nY2FyZGRkZXNj">A Simulation Study</span></p>
</div>
</section>
</section>

</main> <!-- /main -->
<script id = "quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->

</body>

</html>
