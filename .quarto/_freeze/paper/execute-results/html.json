{
  "hash": "e4db5854bf01089ffa71494aaa090f51",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Comparing Internal Validation Methods for Clinical Prediction Models\"\nsubtitle: \"A Simulation Study\"\nauthor: \n  - name: \"Your Name\"\n    affiliation: \"Your Institution\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    theme: cosmo\n    code-fold: true\n    code-tools: false\n  docx:\n    reference-doc: custom-reference.docx\n  pdf:\n    documentclass: article\n    geometry: margin=1in\nexecute:\n  echo: false\n  warning: false\n  message: false\n---\n\n\n\n\n\n\n\n# Abstract {.unnumbered}\n\n**Background**: Internal validation is essential for assessing the likely performance of clinical prediction models in new patients, yet multiple methods exist with little guidance on which performs best.\n\n**Methods**: We conducted a Monte Carlo simulation study comparing three internal validation methods (sample splitting, 10-fold cross-validation, and bootstrap optimism correction) against external validation as the gold standard. We simulated 200 clinical prediction models using logistic regression with 10 predictors and 15% outcome prevalence. Development sample size (n=1,038) was determined using principled criteria (Riley et al., Pavlou et al.) via the samplesizedev package. External validation used n=100,000 to represent asymptotic truth. Performance was assessed using discrimination (AUC), calibration (calibration slope), and overall prediction accuracy (Brier score, MAPE).\n\n**Results**: Bootstrap validation and cross-validation provided nearly unbiased estimates of external performance across all metrics, with exceptionally stable calibration slope estimates (bootstrap SD=0.021, cross-validation SD=0.041). Sample splitting showed extreme variability for calibration slope (SD=0.287 - over 14 times higher than bootstrap), making it unreliable for calibration assessment. All internal methods corrected the optimism observed in apparent validation. Bootstrap showed the best overall performance with minimal bias across metrics (AUC bias: 0.000; calibration slope bias: 0.008) and the lowest RMSE for calibration slope (0.124).\n\n**Conclusions**: For clinical prediction models with moderate sample sizes, bootstrap validation provides the most reliable, stable estimates of external performance, particularly for calibration. Cross-validation with pooled predictions is an acceptable alternative. Sample splitting should be avoided due to extreme variability in calibration estimates and inefficient data use.\n\n**Keywords**: clinical prediction models, internal validation, bootstrap, cross-validation, sample splitting, simulation study\n\n# Introduction\n\n## Background\n\nClinical prediction models are increasingly used to inform medical decision-making, providing individualized risk estimates for outcomes such as disease occurrence, treatment response, or prognosis [@steyerberg2019; @moons2015]. However, models often perform worse in new patients than in the data used for development—a phenomenon known as optimism or overfitting [@harrell2015; @steyerberg2001].\n\nInternal validation techniques aim to provide realistic estimates of model performance in new patients without requiring separate external datasets [@steyerberg2019; @collins2015]. Despite their importance, little empirical guidance exists on which internal validation method performs best under realistic clinical scenarios.\n\n## Internal Validation Methods\n\nThree primary internal validation approaches are commonly used:\n\n**Sample Splitting**: Randomly dividing data into training (typically 60-80%) and test (20-40%) sets. The model is developed on the training set and evaluated on the held-out test set [@altman2009; @bleeker2003]. While conceptually simple, this approach has known limitations including inefficient data use and high variability due to random split selection [@steyerberg2019].\n\n**Cross-Validation**: Dividing data into k folds (typically k=10), iteratively training on k-1 folds and testing on the remaining fold [@stone1974; @hastie2009]. Predictions from all folds are pooled to calculate performance metrics. Cross-validation uses all data for both training and testing, potentially providing more stable estimates than sample splitting [@steyerberg2010].\n\n**Bootstrap Optimism Correction**: Generating bootstrap samples with replacement, calculating performance in both bootstrap samples and original data, and correcting for the average optimism [@efron1983; @efron1993]. Harrell's method, implemented in the rms package, is considered the gold standard bootstrap approach for clinical prediction models [@harrell2015; @harrell2023].\n\n## Gaps in Current Knowledge\n\nWhile theoretical properties of these methods are well-established, empirical comparisons of their performance in realistic clinical scenarios are limited. Previous simulation studies have often:\n\n- Focused on a single performance metric (typically discrimination)\n- Used small numbers of simulations\n- Not compared all three methods simultaneously\n- Not evaluated performance across multiple metrics simultaneously\n\nFurthermore, questions remain about:\n\n1. The stability of different methods across performance metrics\n2. The optimal approach for aggregating cross-validation results\n3. Whether bootstrap validation justifies its computational cost\n4. How sample splitting compares when validation sample size is constrained\n\n## Study Objectives\n\nWe aimed to compare the performance of sample splitting, cross-validation, and bootstrap validation in estimating external validation performance across multiple metrics using a comprehensive simulation study with realistic sample sizes and 200 replications.\n\n# Methods\n\n## Data Generating Process\n\nWe simulated clinical prediction scenarios using a logistic regression model with binary outcome and 10 continuous predictors, all standardized to N(0,1). This represents a moderately complex clinical model with predictors exhibiting various effect sizes.\n\n### Intercept Calculation for Target Prevalence\n\nTo ensure exactly 15% outcome prevalence, we derived the intercept (α) using numerical root-finding. We generated a large sample dataset (n=10,000) with predictors X ~ N(0,1), defined the coefficient vector β with mixed effect sizes, and used the `uniroot()` function to solve for α such that E[expit(α + X'β)] = 0.15.\n\nIn R:\n```r\n# Generate sample data\nX <- replicate(10, rnorm(10000))\nbeta <- c(0.45, 0.40, -0.35, 0.30, -0.25, 0.20, 0.15, 0.10, 0.08, 0.05)\ntarget_prev <- 0.15\n\n# Find alpha (intercept) that gives target prevalence\nfind_alpha <- function(a) {\n  mean(expit(a + as.vector(X %*% beta))) - target_prev\n}\nalpha <- uniroot(find_alpha, c(-10, 10))$root\n```\n\nThis approach ensures the expected prevalence is exactly 15%, accounting for the nonlinear relationship between the linear predictor and outcome probability.\n\n### Data Generating Model\n\nThe true data-generating model was:\n\n$$\n\\text{logit}(P(\\text{outcome}=1)) = -1.9563 + 0.45X_1 + 0.40X_2 - 0.35X_3 + 0.30X_4 - 0.25X_5 + 0.20X_6 + 0.15X_7 + 0.10X_8 + 0.08X_9 + 0.05X_{10}\n$$\n\nwhere all $X_1$ through $X_{10} \\sim N(0,1)$.\n\nThe effect sizes range from moderate (β=0.45 for $X_1$) to small (β=0.05 for $X_{10}$), representing typical clinical prediction scenarios with a mix of strong and weak predictors. This configuration yields exactly 15% outcome prevalence and moderate discrimination (C-statistic ≈0.75).\n\n## Sample Size Determination\n\nRather than using an arbitrary development sample size, we determined the required sample size using principled criteria based on Riley et al. (2020) and Pavlou et al. (2022), implemented in the `samplesizedev` R package [@riley2020; @pavlou2022]. This approach ensures adequate sample size for developing prediction models with minimal optimism.\n\nThe sample size calculation considered:\n\n- **Number of predictors (p)**: 10 candidate parameters\n- **Anticipated outcome prevalence (φ)**: 15%\n- **Target C-statistic (c)**: 0.75 (acceptable discrimination)\n- **Target calibration slope (S)**: 0.90 (accounting for expected shrinkage due to overfitting)\n\nUsing the `samplesizedev()` function, this yielded a required development sample size of **n=1038 total observations** (approximately 156 events), corresponding to 15.6 events per variable (EPV). This sample size is designed to achieve:\n\n1. Small optimism in predictor effect estimates (≤10%)\n2. Precise estimation of overall outcome risk  \n3. Precise estimation of model discrimination\n4. Target calibration accounting for expected shrinkage\n\nThis principled approach ensures our simulation reflects realistic sample sizes for clinical prediction model development, rather than arbitrary choices.\n\n## Simulation Structure\n\nFor each of 200 Monte Carlo iterations:\n\n1. **Development dataset**: Generated n=1038 observations from the data-generating process\n2. **Model development**: Fitted logistic regression model with all 10 predictors\n3. **Apparent validation**: Calculated performance on the development data (resubstitution)\n4. **Internal validation**: Applied three methods:\n   - **Sample Split**: 70/30 random split, single evaluation on 30% test set\n   - **10-fold Cross-Validation**: Stratified folds, pooled predictions across all folds\n   - **Bootstrap**: 200 bootstrap samples, Harrell's optimism correction method\n5. **External validation**: Generated independent n=100,000 observations, evaluated model performance (gold standard representing asymptotic \"truth\")\n\nThe large external validation sample (100,000 observations) ensures minimal Monte Carlo error in estimating true model performance, providing a stable benchmark for comparison.\n\nAll simulations were parallelized using R's `parallel` package to leverage multiple CPU cores. Seeds were set for reproducibility.\n\n## Performance Metrics\n\nWe evaluated four complementary performance metrics:\n\n**Discrimination - AUC (C-statistic)**: Area under the receiver operating characteristic curve, measuring the model's ability to distinguish between patients with and without the outcome [@hanley1982]. Values range from 0.5 (no discrimination) to 1.0 (perfect discrimination). AUC ≥0.7 is generally considered acceptable.\n\n**Calibration - Calibration Slope**: Regression coefficient when modeling true outcomes against logistic predictions [@cox1958; @vancalster2019]. A slope of 1.0 indicates perfect calibration; slopes <1.0 indicate overfitting (predictions too extreme); slopes >1.0 indicate underfitting. This metric directly measures the degree of overfitting/optimism.\n\n**Overall Accuracy - Brier Score**: Mean squared difference between predicted probabilities and observed outcomes [@brier1950]. Lower values indicate better predictions. For binary outcomes, Brier scores typically range from 0 to 0.25.\n\n**Overall Accuracy - MAPE**: Mean absolute prediction error, the average absolute difference between predicted probabilities and observed outcomes. This metric provides an interpretable scale (e.g., MAPE=0.10 means predictions are off by 10 percentage points on average).\n\n# Results\n\n## Summary Statistics\n\n@tbl-summary presents the mean and standard deviation of performance metrics across 200 simulations for each validation approach.\n\n\n\n\n\n::: {#tbl-summary .cell tbl-cap='Performance Metric Summary Statistics Across 200 Simulations'}\n::: {.cell-output-display}\n\n\n|Method           |AUC Mean (SD) |Cal. Slope Mean (SD) |Brier Mean (SD) |MAPE Mean (SD) |\n|:----------------|:-------------|:--------------------|:---------------|:--------------|\n|Apparent         |0.728 (0.022) |1.000 (0.000)        |0.116 (0.007)   |0.232 (0.014)  |\n|Bootstrap        |0.704 (0.025) |0.879 (0.021)        |0.120 (0.007)   |0.235 (0.014)  |\n|Cross-validation |0.697 (0.026) |0.817 (0.041)        |0.120 (0.007)   |0.235 (0.014)  |\n|Sample Split     |0.697 (0.048) |0.835 (0.287)        |0.121 (0.014)   |0.235 (0.016)  |\n|External         |0.704 (0.007) |0.871 (0.107)        |0.119 (0.001)   |0.235 (0.007)  |\n\n\n:::\n:::\n\n\n\n\n\nApparent validation showed optimistic performance across all metrics, with perfect calibration slope (mean=1.000) by definition. All three internal validation methods successfully corrected for this optimism, though to varying degrees.\n\nBootstrap validation showed the most stable calibration slope estimates (SD=0.021), followed by cross-validation (SD=0.041). Sample splitting showed extremely high variability (SD=0.287), more than 14 times higher than bootstrap validation.\n\n## Bias and RMSE\n\n@tbl-bias shows the bias and root mean squared error (RMSE) of each internal validation method relative to external validation.\n\n\n\n\n\n::: {#tbl-bias .cell tbl-cap='Bias and RMSE Relative to External Validation'}\n::: {.cell-output-display}\n\n\n|Method           |AUC Bias (RMSE) |Cal. Slope Bias (RMSE) |Brier Bias (RMSE) |MAPE Bias (RMSE) |\n|:----------------|:---------------|:----------------------|:-----------------|:----------------|\n|Bootstrap        |0.000 (0.024)   |0.008 (0.124)          |0.000 (0.007)     |0.000 (0.008)    |\n|Cross-validation |-0.007 (0.026)  |-0.054 (0.149)         |0.000 (0.007)     |0.000 (0.008)    |\n|Sample Split     |-0.007 (0.048)  |-0.036 (0.311)         |0.001 (0.014)     |-0.000 (0.012)   |\n\n\n:::\n:::\n\n\n\n\n\n::: {.callout-note}\nNegative bias indicates the internal method underestimates external performance (pessimistic); positive bias indicates overestimation (optimistic).\n:::\n\nBootstrap validation showed minimal bias across all metrics, with particularly strong performance for calibration slope (bias=0.008, RMSE=0.124). Cross-validation slightly underestimated calibration slope but maintained reasonable RMSE. Sample splitting showed the largest RMSE for calibration slope due to extreme variability, despite near-zero bias on average.\n\n## Visual Comparison\n\nThe following figures show the distribution of performance metrics across all validation approaches. Each plot displays five boxplots representing: Apparent validation (baseline, optimistic), Sample Split (internal), Cross-validation (internal), Bootstrap (internal), and External validation (gold standard).\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![AUC (C-statistic) across validation approaches. Higher values indicate better discrimination.](comparison_auc.png){#fig-auc}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Calibration slope across validation approaches. Values of 1.0 indicate perfect calibration; values <1.0 indicate overfitting.](comparison_calibration.png){#fig-calibration}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Brier score across validation approaches. Lower values indicate better overall prediction accuracy.](comparison_brier.png){#fig-brier}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Mean Absolute Prediction Error (MAPE) across validation approaches. Lower values indicate better prediction accuracy.](comparison_mape.png){#fig-mape}\n:::\n:::\n\n\n\n\n\n@fig-calibration clearly illustrates the extreme variability of sample splitting for calibration slope assessment, with the interquartile range spanning a much wider range than bootstrap or cross-validation methods.\n\n# Discussion\n\n## Principal Findings\n\nThis comprehensive simulation study comparing internal validation methods for clinical prediction models yielded three key findings:\n\n1. **Bootstrap validation provides the most stable and accurate estimates** across all performance metrics, particularly for calibration slope where it showed minimal bias (0.008) and the lowest RMSE (0.124).\n\n2. **Cross-validation is an acceptable alternative**, providing reasonably stable estimates with slightly more pessimistic bias for calibration slope but good overall performance.\n\n3. **Sample splitting should be avoided** due to extreme variability in calibration slope estimates (SD 14 times higher than bootstrap), making it unreliable despite low average bias.\n\n## Comparison with Existing Literature\n\nOur findings align with and extend the seminal work of Smith et al. (2014) [@smith2014], who compared internal validation methods using Down syndrome screening and cesarean delivery prediction data. Smith et al. found that bootstrap validation, 10-fold cross-validation with 20 replications, and leave-pair-out cross-validation all produced unbiased estimates, while sample splitting and cross-validation without replication showed bias and/or greater absolute errors.\n\nOur study provides complementary evidence by:\n\n1. **Using principled sample size determination**: Rather than arbitrary sample sizes, we used the `samplesizedev` package to ensure adequate EPV (15.6), compared to Smith et al.'s EPV of 5 which was \"intentionally well below the generally recommended EPV of 10\" to simulate challenging scenarios.\n\n2. **Comprehensive multi-metric evaluation**: While Smith et al. focused primarily on the C-statistic, we evaluated discrimination (AUC), calibration (calibration slope), and overall accuracy (Brier score, MAPE) simultaneously, revealing that the extreme variability of sample splitting is particularly problematic for calibration assessment.\n\n3. **Larger simulation study**: Our 200 replications with 100,000 external validation observations per simulation provide more stable estimates of method performance than previous studies.\n\n4. **Consistent recommendations**: Both studies strongly recommend bootstrap validation as the primary method and identify sample splitting as problematic. However, our findings suggest that for calibration assessment specifically, the instability of sample splitting is even more severe than for discrimination.\n\nOur results for cross-validation without replication differ slightly from Smith et al., who found it had \"greater absolute errors\" but we find it provides reasonably unbiased estimates with acceptable (though suboptimal) variability. This likely reflects our higher EPV (15.6 vs. 5) and the specific focus on calibration slope where cross-validation's pessimistic bias may be less problematic than sample splitting's high variance.\n\n## Implications for Practice\n\nFor researchers developing clinical prediction models with moderate sample sizes (n≈1,000), we recommend:\n\n1. **First choice**: Bootstrap optimism correction (200+ bootstrap samples) using established implementations like the `rms` package\n2. **Acceptable alternative**: 10-fold cross-validation with pooled predictions\n3. **Avoid**: Sample splitting, particularly for calibration assessment\n\nThe computational cost of bootstrap validation (higher than sample splitting but similar to cross-validation) is justified by its superior performance and stability.\n\n## Limitations\n\nOur study has several limitations:\n\n1. We simulated only one scenario (10 predictors, 15% prevalence, moderate discrimination). Performance may differ with different parameters.\n2. We used logistic regression; non-linear models may behave differently.\n3. We assumed independent predictors; correlated predictors might affect relative performance.\n4. Our sample size (n=1,038) represents moderate-sized studies; very small or very large samples may show different patterns.\n\n## Future Directions\n\nFuture research should investigate:\n\n- Performance across a wider range of sample sizes, prevalence rates, and effect sizes\n- Comparison with other validation methods (e.g., .632+ bootstrap)\n- Performance with non-linear models (random forests, neural networks)\n- Impact of predictor correlation structures\n- Optimal number of bootstrap samples and cross-validation folds\n\n# Conclusions\n\nFor clinical prediction models with moderate sample sizes, bootstrap validation provides the most reliable, stable estimates of external performance, particularly for calibration. Cross-validation with pooled predictions is an acceptable alternative. Sample splitting should be avoided due to extreme variability in calibration estimates and inefficient data use.\n\n# References {.unnumbered}\n\n::: {#refs}\n:::\n\n# Appendix {.unnumbered}\n\n## Simulation Parameters\n\n**Table A1. Simulation Parameters and Data Generating Process**\n\n| Parameter | Value/Specification |\n|-----------|---------------------|\n| Number of simulations | 200 |\n| Development sample size | 1,038 (calculated via samplesizedev) |\n| Expected events in development | ~156 (15% of development sample) |\n| External validation sample size | 100,000 |\n| Outcome prevalence | 15% (exact) |\n| Number of predictors | 10 (all continuous, standardized N(0,1)) |\n| Model type | Logistic regression |\n| Sample size criteria | Riley et al. (2020), Pavlou et al. (2022) |\n| Target C-statistic | 0.75 |\n| Target calibration slope | 0.90 (after shrinkage) |\n| Bootstrap samples | 200 |\n| Cross-validation folds | 10 (stratified) |\n| Sample split ratio | 70% train / 30% test |\n\n## Code Availability\n\nAll simulation code and data are available at [GitHub repository URL].\n\n## Computational Details\n\n- Platform: [System specifications]\n- R version: 4.3.0\n- Parallelization: 7 cores using `parallel::mclapply`\n- Mean runtime per simulation: 3.8 minutes\n- Total computation time: ~12.7 hours\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}