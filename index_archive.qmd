---
title: "Comparing Internal Validation Methods for Clinical Risk Prediction"
subtitle: "A Simulation Study"
author: "Ashley Dickson"
date: today
format:
  html:
    toc: true
    toc-depth: 2
    number-sections: true
    theme: cosmo
    code-fold: true
    code-tools: false
    embed-resources: false
execute:
  echo: false
  warning: false
  message: false
---

```{r setup}
#| include: false

# Load required libraries
library(knitr)
library(dplyr)
library(tidyr)
library(gt)

# Load results
results <- read.csv("validation_results.csv")

# Calculate summary statistics for each method
summary_stats <- results %>%
  group_by(method) %>%
  summarise(
    # AUC
    auc_mean = mean(internal_auc),
    auc_sd = sd(internal_auc),
    # Calibration Slope
    cal_slope_mean = mean(internal_cal_slope),
    cal_slope_sd = sd(internal_cal_slope),
    # Brier Score
    brier_mean = mean(internal_brier),
    brier_sd = sd(internal_brier),
    # MAPE
    mape_mean = mean(internal_mape),
    mape_sd = sd(internal_mape)
  )

# Get apparent values (same for all methods, just take first)
apparent_stats <- results %>%
  filter(method == "Sample Split") %>%
  summarise(
    auc_mean = mean(apparent_auc),
    auc_sd = sd(apparent_auc),
    cal_slope_mean = mean(apparent_cal_slope),
    cal_slope_sd = sd(apparent_cal_slope),
    brier_mean = mean(apparent_brier),
    brier_sd = sd(apparent_brier),
    mape_mean = mean(apparent_mape),
    mape_sd = sd(apparent_mape)
  ) %>%
  mutate(method = "Apparent")

# Get external values 
external_stats <- results %>%
  filter(method == "Sample Split") %>%
  summarise(
    auc_mean = mean(external_auc),
    auc_sd = sd(external_auc),
    cal_slope_mean = mean(external_cal_slope),
    cal_slope_sd = sd(external_cal_slope),
    brier_mean = mean(external_brier),
    brier_sd = sd(external_brier),
    mape_mean = mean(external_mape),
    mape_sd = sd(external_mape)
  ) %>%
  mutate(method = "External")

# Combine all statistics
all_stats <- bind_rows(apparent_stats, summary_stats, external_stats)

# Calculate bias and RMSE
bias_rmse <- results %>%
  group_by(method) %>%
  summarise(
    auc_bias = mean(internal_auc - external_auc),
    auc_rmse = sqrt(mean((internal_auc - external_auc)^2)),
    cal_slope_bias = mean(internal_cal_slope - external_cal_slope),
    cal_slope_rmse = sqrt(mean((internal_cal_slope - external_cal_slope)^2)),
    brier_bias = mean(internal_brier - external_brier),
    brier_rmse = sqrt(mean((internal_brier - external_brier)^2)),
    mape_bias = mean(internal_mape - external_mape),
    mape_rmse = sqrt(mean((internal_mape - external_mape)^2))
  )

# Extract key values for inline text
bootstrap_cal_sd <- all_stats %>% filter(method == "Bootstrap") %>% pull(cal_slope_sd)
cv_cal_sd <- all_stats %>% filter(method == "Cross-validation") %>% pull(cal_slope_sd)
split_cal_sd <- all_stats %>% filter(method == "Sample Split") %>% pull(cal_slope_sd)
bootstrap_auc_bias <- bias_rmse %>% filter(method == "Bootstrap") %>% pull(auc_bias)
bootstrap_cal_bias <- bias_rmse %>% filter(method == "Bootstrap") %>% pull(cal_slope_bias)
bootstrap_cal_rmse <- bias_rmse %>% filter(method == "Bootstrap") %>% pull(cal_slope_rmse)
split_bootstrap_ratio <- split_cal_sd / bootstrap_cal_sd

# CALCULATE DGP PARAMETERS FROM CODE
# (These match what's in simulation_code.R)

# Expit function
expit <- function(x) 1 / (1 + exp(-x))

# Generate sample to calculate intercept
set.seed(123)
n_sample <- 10000
X_sample <- replicate(10, rnorm(n_sample))

# Beta coefficients (from simulation)
beta <- c(0.45, 0.40, -0.35, 0.30, -0.25, 0.20, 0.15, 0.10, 0.08, 0.05)
target_prev <- 0.15

# Calculate intercept
find_alpha <- function(a) {
  mean(expit(a + as.vector(X_sample %*% beta))) - target_prev
}
alpha <- uniroot(find_alpha, c(-10, 10))$root

# Calculate sample size using samplesizedev (matching simulation_code.R)
library(samplesizedev)

sample_size_params <- list(
  p = length(beta),        # 10 predictors
  phi = target_prev,       # 15% prevalence
  c = 0.75,               # Target C-statistic
  S = 0.90                # Target calibration slope (accounting for shrinkage)
)

res <- samplesizedev(
  p = sample_size_params$p,
  phi = sample_size_params$phi,
  c = sample_size_params$c,
  S = sample_size_params$S
)

# Use Pavlou et al. simulation method (as in simulation_code.R)
n_dev <- ceiling(res$sim)
n_events <- ceiling(n_dev * sample_size_params$phi)
epv <- n_events / length(beta)
n_ext <- 100000
```

# Overview

This simulation study compares three internal validation methods (sample splitting, cross-validation, and bootstrap) for clinical prediction models. It uses 200 simulations with realistic sample sizes (n=`r n_dev`) and evaluates performance across several metrics.

::: {.callout-note appearance="simple"}
## Key Findings

Bootstrap validation performs best overall, with minimal bias and exceptional stability, especially for calibration assessment.

Sample splitting is unreliable - it shows extreme variability in calibration slope estimates (SD `r round(split_bootstrap_ratio)` times higher than bootstrap), making it unsuitable for calibration assessment.

Cross-validation is acceptable as an alternative to bootstrap, providing reasonably stable estimates with slight pessimistic bias.

Main recommendation: Use bootstrap validation (200+ samples) for internal validation. Cross-validation is acceptable if bootstrap is too computationally intensive. Avoid sample splitting.
:::

# Background

Clinical prediction models often perform worse in new patients than in development data due to overfitting. Internal validation provides realistic performance estimates without requiring external datasets. However, multiple methods exist with limited guidance on which performs best.

Study aim: Compare sample splitting, 10-fold cross-validation, and bootstrap optimism correction against external validation (gold standard) across discrimination, calibration, and overall accuracy metrics.

# Methods

## Data Generating Process

Simulated scenario: - Binary outcome with 15% prevalence - 10 continuous predictors (X₁...X₁₀), all \~N(0,1) - Logistic regression with mixed effect sizes - Intercept (α = -1.956) calculated to achieve exactly 15% prevalence

Model equation:

$$
\text{logit}(P(\text{outcome}=1)) = -1.956 + 0.45X_1 + 0.40X_2 - 0.35X_3 + ... + 0.05X_{10}
$$

## Sample Sizes

-   Development: n=`r n_dev` (`r n_events` events, EPV=`r sprintf("%.1f", epv)`)
    -   Determined using `samplesizedev` package
-   External validation: n=`r format(n_ext, big.mark=",")` (asymptotic truth)
-   Simulations: 200 iterations

## Methods Compared

For each simulation:

1.  Apparent validation - Optimistic test on development data (baseline)
2.  Sample Split - 70/30 split
3.  10-fold Cross-validation - Stratified folds with pooled predictions
4.  Bootstrap - Harrell's optimism correction (200 resamples)
5.  External validation - Independent large dataset (gold standard)

## Performance Metrics

-   Discrimination: AUC (C-statistic)
-   Calibration: Calibration slope (1.0 = perfect; \<1.0 = overfitting)
-   Overall accuracy: Brier score, Mean Absolute Prediction Error (MAPE)

# Results

## Performance Summary

@tbl-summary shows performance across all validation approaches. Apparent validation is optimistic (as expected), while all internal methods successfully correct for this optimism to varying degrees.

```{r}
#| label: tbl-summary
#| tbl-cap: "Mean (SD) of performance metrics across 200 simulations"

all_stats %>%
  mutate(
    Method = method,
    `AUC` = sprintf("%.3f (%.3f)", auc_mean, auc_sd),
    `Cal. Slope` = sprintf("%.3f (%.3f)", cal_slope_mean, cal_slope_sd),
    `Brier` = sprintf("%.3f (%.3f)", brier_mean, brier_sd),
    `MAPE` = sprintf("%.3f (%.3f)", mape_mean, mape_sd)
  ) %>%
  select(Method, AUC, `Cal. Slope`, Brier, MAPE) %>%
  kable()
```

Bootstrap and cross-validation show much lower standard deviations than sample splitting, particularly for calibration slope (Bootstrap SD: `r sprintf("%.3f", bootstrap_cal_sd)` vs. Sample Split SD: `r sprintf("%.3f", split_cal_sd)`).

## Bias and Precision

@tbl-bias compares internal methods to external validation. Lower bias and RMSE indicate better performance.

```{r}
#| label: tbl-bias
#| tbl-cap: "Bias and RMSE relative to external validation"

bias_rmse %>%
  mutate(
    Method = method,
    `AUC` = sprintf("%.3f (%.3f)", auc_bias, auc_rmse),
    `Cal. Slope` = sprintf("%.3f (%.3f)", cal_slope_bias, cal_slope_rmse),
    `Brier` = sprintf("%.3f (%.3f)", brier_bias, brier_rmse),
    `MAPE` = sprintf("%.3f (%.3f)", mape_bias, mape_rmse)
  ) %>%
  select(Method, AUC, `Cal. Slope`, Brier, MAPE) %>%
  kable()
```

Bootstrap shows minimal bias across all metrics - Cross-validation slightly underestimates calibration slope (pessimistic) - Sample splitting has low bias on average but very high RMSE due to extreme variability

## Visual Comparison

The following plots show the distribution of each metric across 200 simulations:

```{r}
#| label: fig-auc
#| fig-cap: "AUC (C-statistic) distributions"
#| fig-width: 10
#| fig-height: 6

knitr::include_graphics("comparison_auc.png")
```

```{r}
#| label: fig-calibration
#| fig-cap: "Calibration slope distributions. Note the extreme variability of sample splitting."
#| fig-width: 10
#| fig-height: 6

knitr::include_graphics("comparison_calibration.png")
```

```{r}
#| label: fig-brier
#| fig-cap: "Brier score distributions: lower is better."
#| fig-width: 10
#| fig-height: 6

knitr::include_graphics("comparison_brier.png")
```

```{r}
#| label: fig-mape
#| fig-cap: "Mean Absolute Prediction Error: lower is better."
#| fig-width: 10
#| fig-height: 6

knitr::include_graphics("comparison_mape.png")
```

@fig-calibration  demonstrates why sample splitting is problematic: the wide spread of calibration slope estimates makes it unreliable for calibration assessment, even though the average bias is small.

# Discussion

## Main Findings

Bootstrap validation consistently outperformed other methods: - Minimal bias across all metrics (AUC bias: `r sprintf("%.3f", bootstrap_auc_bias)`, calibration slope bias: `r sprintf("%.3f", bootstrap_cal_bias)`) - Most stable estimates (especially calibration slope SD: `r sprintf("%.3f", bootstrap_cal_sd)`) - Lowest RMSE for calibration slope (`r sprintf("%.3f", bootstrap_cal_rmse)`)

Cross-validation performed well as a computationally simpler alternative, with slight pessimistic bias for calibration but acceptable variability.

Sample splitting was problematic due to extreme variability (calibration slope SD `r round(split_bootstrap_ratio)`× higher than bootstrap), making individual estimates unreliable despite low average bias across many simulations.

## Comparison with Smith et al. (2014)

These findings align with Smith et al.'s  study comparing internal validation methods. Both studies found: 
  - Bootstrap, cross-validation (with replication) methods provide unbiased estimates 
  - Sample splitting shows problematic performance 
  - Bootstrap validation is recommended as the primary approach

This study differs from their work by: 
  1. Higher EPV: We used EPV=`r sprintf("%.1f", epv)` (via principled sample size calculation) vs. their EPV=5 (intentionally low to test challenging scenarios) 
  2. Multi-metric focus: While Smith et al. emphasized the C-statistic, we evaluated discrimination, calibration, and overall accuracy simultaneously 
  3. Calibration emphasis: We highlight that sample splitting's variability problem is particularly severe for calibration assessment

The consistency between studies strengthens the evidence for bootstrap validation in clinical prediction modeling.

## Practical Recommendations

For researchers developing prediction models:

1.  Primary recommendation: Use bootstrap optimism correction (≥200 samples) via established implementations (e.g., `rms` package)
2.  Alternative: 10-fold cross-validation with pooled predictions is acceptable if bootstrap is computationally prohibitive
3.  Avoid: Sample splitting for calibration assessment due to extreme variability

## Limitations

-   Single scenario (10 predictors, 15% prevalence, moderate discrimination)
-   Logistic regression only (non-linear models may behave differently)
-   Independent predictors (correlation might affect performance)
-   Moderate sample size (n≈1,000); very small/large samples may show different patterns

## Conclusions

For clinical prediction models with moderate sample sizes, bootstrap validation provides the most reliable estimates of external performance. Cross-validation is an acceptable alternative. Sample splitting should be avoided, especially for calibration assessment, due to extreme variability that makes individual estimates unreliable.

------------------------------------------------------------------------

## Code and Data Availability

All simulation code and generated data are available at \[https://github.com/AshleyMDickson/internal_validation_paper.git\].
