{"entries":[{"caption":"Brier score across validation approaches. Lower values indicate better overall prediction accuracy.","key":"fig-brier","order":{"number":3,"section":[3,3,0,0,0,0,0]}},{"caption":"Performance Metric Summary Statistics Across 200 Simulations","key":"tbl-summary","order":{"number":1,"section":[3,1,0,0,0,0,0]}},{"caption":"Calibration slope across validation approaches. Values of 1.0 indicate perfect calibration; values <1.0 indicate overfitting.","key":"fig-calibration","order":{"number":2,"section":[3,3,0,0,0,0,0]}},{"caption":"Mean Absolute Prediction Error (MAPE) across validation approaches. Lower values indicate better prediction accuracy.","key":"fig-mape","order":{"number":4,"section":[3,3,0,0,0,0,0]}},{"caption":"Bias and RMSE Relative to External Validation","key":"tbl-bias","order":{"number":2,"section":[3,2,0,0,0,0,0]}},{"caption":"AUC (C-statistic) across validation approaches. Higher values indicate better discrimination.","key":"fig-auc","order":{"number":1,"section":[3,3,0,0,0,0,0]}}],"headings":["abstract","introduction","background","internal-validation-methods","gaps-in-current-knowledge","study-objectives","methods","data-generating-process","intercept-calculation-for-target-prevalence","data-generating-model","sample-size-determination","simulation-structure","performance-metrics","results","summary-statistics","bias-and-rmse","visual-comparison","discussion","principal-findings","comparison-with-existing-literature","implications-for-practice","limitations","future-directions","conclusions","references","appendix","simulation-parameters","code-availability","computational-details"]}